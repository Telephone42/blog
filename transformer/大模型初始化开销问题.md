## MegaScale

是一种用于训练规模超过10000个GPU的LLM生产系统，采用全栈的方法，协同设计了算法和系统组建，横跨model block、optimizer design、computation and communication overlapping、operator optimization、data pipline、network performance tuning等方面

in-depth observability 是解决大规模训练问题的关键，于是开发了一套诊断工具，用于监测堆栈深处的系统

We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers

MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34× compared to Megatron-LM