



### 什么是GPT？

GPT 的含义：Generative Pre-trained Transformer

其中FGenerative的意思是“生成性的”，例如机器人可以生成新文本，pre-trained是预训练，指的是模型经历了大量数据的学习，前面的pre前缀，则暗示了模型能够针对具体任务，通过额外训练来进行微调，而transformer则是这个概念的主角。


### Transformer

Transformer是一种特殊的神经网络，一种特殊的机器学习模型

最原版的transformer是谷歌在2017年推出的，即论文attention is all you need。原版的transformer专注于把一种语言的文本翻译成另一种

我们现在关注的是transformer的变体，即构建chatgpt等工具模型后背的模型。这种变体模型的输入是一段文本，或者伴随一些图像和音频（多模态），预测出文段接下来的内容，即输出为接下来不同文本片段的概率分布。

不过我们希望的是生成一段文本，而不是一个单词片段，所以需要将该模型运行一次的输出结果附加到输入文本的末端，对其再进行一次输出，如此迭代，就可以生成一段文本。


现在我们先预览一下数据是如何在transformer中流动的，由于是预览，只列出一些主要步骤：

<ol>
<li>首先，由于输入是一段文本，我们需要先将文本切成许多小片段，这些小片段被称为token，但并不是说token就是指单词片段，对于其他模态的输入，例如音频视频，token即为音频片段和图像像素块。每一个token会与一个向量关联，关联的目的是为了给这些片段（token）进行编码。这时也许会有人说，为什么要编码？字符不是原本就有编码吗？那是因为字符本身的编码所含的信息量太少了，以ASCII码为例，一个字符才一个字节，那么一个token不过才几字节而已，而且还只有一个维度。由于信息量太少，无法包含一个词的语义甚至色彩，到后面我们会发现，两个词意思相近的话，它们所关联的向量的欧式距离会很短，而这是ASCII码无法做到的</li>
<li>随后，这些向量会经过“注意力模块”（Attention block），在这个模块中，这些向量可以相互“交流”，通过相互传递信息，来更新自己的值。那么为什么要更新呢？这是因为一个词的意思在不同语境下的含义是不一样的，比如数学中的“23模7等于2”和“这个向量的模为23”中的“模”的意思是完全不同的，前者是指“模运算”，后者则是“模长”。所以一个token所关联的向量需要在语境不同的情况下进行更新</li>
<li>之后，这些向量会经过另一种处理，取决于不同资料，有的会把它叫“多层感知器”（multi-layer perception），有的会称为“前馈层”（feed-forword layer），在这个阶段，向量之间不会再“交流”，而是并行经历同一处理。这个过程可以看作是对每个向量提出一系列问题，然后根据这些问题的答案来更新向量</li>
<li>然后重复步骤2、3，即注意力模块与多层感知器模块层层堆叠</li>
<li>最后，将整段文字所有关键含义以某种方式，融入到序列的最后一个向量。对该向量进行某种操作后，得出所有可能token的概率分布</li>
</ol>

现在我们已经可以做到，输入某个确定文本就可以预测下一个词，那么将这个词追加到文本末尾后形成新的文本，在进行同样的步骤，那么就可以一直生成完整个文本。


但是以上过程与我们所说的聊天式机器人（例如chatgpt）还是有一些区别，毕竟上面的步骤只是在给出一段文本后不断生成后续文字，像是在续写或者是补完故事。

实际上，聊天式机器人也是以同样的方式实现的，例如我们给出一下文本：

````
接下来是一段用户与热心AI助手的对话：

用户：请给我一些做饭的建议

AI助手：
````
根据这样一段输入，上述补全故事的工具就变成了一个聊天机器人。当然，这需要一些额外的训练步骤。

### 深度学习

深度学习是机器学习的一种方法。机器学习是采用数据驱动，反馈到模型参数，去指导模型的行为，而不是传统的程序，


不要试图在代码中明确定义如何执行一个任务，而是去构建一个具有可调节参数的灵活架构，然后拿大量的实例，即给定输入和输出，让模型自己调整各种参数值


GPT-3的参数有1750亿个，但并不是说一味地扩大模型参数量就可以了，有时模型要么对训练数据严重过拟合，要么完全训练不出来


而深度学习在过去的几十年中，展现出了出色的规模化能力，即在大量数据下仍具有很好的泛化性，它们的共同点是都使用了相同的训练方法————反向传播算法（backpropagation）

也就是说，为了能够在如此多的参数下正常运作，模型必须遵循某种特定的结构：

<ol>
<li>输入的格式必须为实数数组，可以是一维数组，可以是二维数组，也可以是更高维，我们称之为张量（tensor）</li>
<li>输入数据通常被逐步转换成多个不同的层，同时，每一层的结构都是实数数组（或者说张量），直到最后一层，就被视为是模型的输出。例如transformer中，最后的输出就是一个token的概率分布向量</li>
<li>深度学习中，这些模型的参数通常被称为“权重”，这是因为这些模型的一个关键特征是，参数与待处理数据之间唯一的交互方式是通过“加权和”的方式，虽然模型中也有一些非线性函数，但它们不依赖参数。通常情况下，加权和不会赤裸裸的写出来，而是写成矩阵向量相乘的形式。例如GPT-3中的1750亿个参数，它们被组织成近28000个矩阵，而这些矩阵又被分为8类，我们的任务就是逐一了解每类矩阵的作用</li>
</ol>


在此我们简单地认为token就是单词

### 嵌入矩阵
模型有一个预设的词汇库，它包含了所有可能的单词，GPT-3有50257个，于是我们遇到了模型中的第一个矩阵，称为嵌入矩阵（Embedding matrix），每个词(严格来说是token)都对应了该矩阵的一列，这些列决定了第一步中，每个单词对应的向量，所以根据这个矩阵的定义，它的行数是向量的长度，在GPT-3中为12288，列数是词汇库中的词汇个数，在这个例子中为502570. 所以它是一个$12288\times50257$ 的一个矩阵（约为6.17亿个数据），我们把这个矩阵记作$W_E$（Embedding matrix），它的初始值随机，但将基于数据进行学习。

### 词嵌入
早在transformer之前，将单词转化为向量就是机器学习中常见的作法，通常称其为“词嵌入”（embedding a word）。这个概念其实可以进行具象化的理解，当模型在训练阶段调整权重，以确定不同单词将如何被嵌入向量时，它们最终嵌入的向量在空间中的方向，往往具有某种语义意义。例如在经过训练的模型中输入与单词tower向量接近的词向量，例如$E(towers)$，$E(gate)$，$E(building)$，$E(skyscraper)$等（$E()$可以看作为一个映射，将单词映射到所关联的向量）时，它们的欧式距离很短，像是被“嵌入”到了某个相似的语义上。

（在实验中，发现$E(queen)-E(king) \approx E(woman)-E(man)$）


### 解嵌入矩阵

在预览的第一步中，我们说要把输入中的每一个token都与一个向量关联起来。由于此时已经有一个词汇库了，那么最开始只能从这些词汇库中把对应词汇（token）的向量拿出来，排列成一个矩阵，暂且称其为上下文矩阵。但这个操作对于每一个token是独立的，也就是现在拼成的这个上下文矩阵是没有结合上下文的结果。况且如果一开始词汇库中的数据是随机的，那么这个新的上下文矩阵的每一列甚至没有包含这个词本身的含义，更谈不上结合上下文。


为了让这些向量进行“交流”，获得更能结合上下文的语义，我们会经过2、3步。由于注意力模块的网络一次只能处理特定数量的向量，被称作为上下文长度（context size），在GPT-3中是2048，因此流经网络的数据只有2048列，每列12288维。上下文的长度限制了transformer在预测下一个词时能结合的文本量，这就是为什么早期的聊天机器人会给人一种“健忘”的感觉。


这里先跳过中间关于注意力机制的细节，直接来到最后。由于我们最终的结果是输出下一个可能的token的概率分布向量，那么这个概率分布是怎么来的呢？这涉及两个步骤：


<ol>
<li>将上下文矩阵中的最后一个向量，即输入文本中最后一个token在经过训练之后所对应的向量（这个向量的维度为12288），映射到一个包含50257个值的列表（list），每个值对应词汇库里的一个token，我们称这些值为logits（因为机器学习专家会将这些未归一化的输出，称作预测下一个词的logits）</li>
<li>然后把这个列表归一化为概率分布，具体做法为输入到softmax函数中，得到概率分布</li>
</ol>


第一步中提到了一个新矩阵，由于这个矩阵可以将向量解码成概率分布（这个说法不准确，因为还要经过一次softmax），所以我们将这个矩阵称为解嵌入矩阵（Unembedding matrix），记作$W_U$，和其他矩阵一样，它的初始值是随机的，但将会在训练的过程中学习。由这个矩阵的作用我们可以得知，它的每一行对应一个词向量（即token所关联的向量），而最后输出的列表是所有token的值，故维数为5027，所以这是一个$12228\times 50257$ 的矩阵，它所含的数据也为6.17亿

这里先挖一个坑，由于最后一次循环得到的上下文矩阵中，所有向量都进行了多次的调整，但为什么却只用最后一个向量来得到结果呢？


### softmax

softmax可以使最大值接近1，较小的值则会非常接近0，它的表达式为：

$$
y_i= \frac{e^{x_i}/T }{{\sum_{n=0}^{N-1}}e^{x_n}/T} 
$$

其中指数可以拉开差距，除法可以使得最终和为1，T被称为温度，温度越高，概率分布会更均匀（还蛮形象的），可以人工进行调节，温度越高虽然可能产生更多的可能性，但同时也意味着风险



the prefix insinuates that there's more room to fine-tune it on specific tasks with addtional training.


## 注意力机制

### 单头注意力机制

### 查询矩阵